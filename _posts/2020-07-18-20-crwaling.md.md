---
layout: post
title: "0718-20 크롤링 완성"
subtitle: "크롤링 완성."
date: 2020-07-20 13:00:00 +0900
background: '/img/posts/06.jpg'
categories: ['programming', 'python']
---

# 0718-20

아래 쪽 Save 함수의 파일 이름과, Run 함수 안의 숫자만 바꿔주면 됨.

[뮤지컬 - 현재 공연중.csv](https://www.notion.so/0718-20-f3f915e68d884d1ab35f54c76958d419#5e183d74b67e42a2989ccfd51ee2ed86)

수정 내역

1. 페이지 수에 맞게 조정할 필요 없이 연도만 정해주면 자동으로 크롤링
2. 페이지 크롤링 한 후 저장하게 함. → 오류가 나도 직전까지의 결과물이 생김  




```python
from selenium import webdriver
from bs4 import BeautifulSoup
import pandas as pd
import time

driver = webdriver.Chrome('/Users/beom/Documents/chromedriver')
driver.get('http://www.playdb.co.kr/playdb/playdblist.asp?sReqMainCategory=000001')
time.sleep(2)

def yearselect(yearvalue): # 현재 공연중=1,개막 예정=2,2019년=3,2018=4, ... , 2007년 이하=15
    yearpage = driver.find_element_by_xpath('//*[@id="contents"]/div[2]/table/tbody/tr[8]/td/table/tbody/tr[2]/td/table/tbody/tr/td[1]/table/tbody/tr/td[2]/table/tbody/tr[2]/td/table/tbody/tr[2]/td/a['+str(yearvalue)+']')
    yearpage.click()
    time.sleep(5)


def GetTotalPage():  # 총 페이지 구하기
    html = driver.page_source
    soup = BeautifulSoup(html, 'html.parser')
    tmp = soup.select('#contents > div.container1 > table > tbody > tr:nth-child(11) > td > table > tbody > tr:nth-child(35)')

    for x in tmp:
        a = (x.text.strip(""))

    b = a.split("/")
    totalpage = b[len(b)-1].strip("]")

    return int(totalpage)

def page(pagevalue): # 페이지 넘기기
    pages = driver.find_element_by_xpath('//*[@id="contents"]/div[2]/table/tbody/tr[11]/td/table/tbody/tr[35]/td/a[' + str(pagevalue) + ']')
    pages.click()


alist = []
def crawling(): # 크롤링
    for i in range(1, 16):
        j = 1 + (i * 2)
        title = driver.find_element_by_xpath('//*[@id="contents"]/div[2]/table/tbody/tr[11]/td/table/tbody/tr[' + str(j) + ']/td/table/tbody/tr/td[1]/table/tbody/tr/td[3]/table/tbody/tr[1]/td/b/font/a')
        tmp = driver.find_element_by_xpath('//*[@id="contents"]/div[2]/table/tbody/tr[11]/td/table/tbody/tr[' + str(j) + ']/td/table/tbody/tr/td[1]/table/tbody/tr/td[3]/table/tbody/tr[2]/td')
        tmp_list = tmp.text.split("\n")
        genre = tmp_list[0][7:]
        date = tmp_list[1]
        loca = tmp_list[2][5:]
        if len(tmp_list) == 3:
            cast = "none"
            staff = "none"
            alist.append([title.text, genre, loca, cast])
        elif len(tmp_list) == 4 and tmp_list[3][0:2] == "출연":
            cast = tmp_list[3][5:]
            staff = "none"
            alist.append([title.text, genre, loca, cast])
        elif len(tmp_list) == 4 and tmp_list[3][0:5] == "Staff":
            cast = "none"
            staff = tmp_list[3][7:]
            alist.append([title.text, genre, loca, cast])
        elif len(tmp_list) == 5 and tmp_list[3][0:2] == "출연":
            cast = tmp_list[3][5:]
            staff = tmp_list[4][7:]
            alist.append([title.text, genre, loca, cast])
    return alist

def Run(yearvalue):   # 실행
    yearselect(yearvalue)
    Number = GetTotalPage()
    NofTimes = Number // 10   # For 문 횟수 정하기 위힌 몫 (나머지 버림)
    remainder = GetTotalPage() % 10 # For 문 횟수 정하기 위한 나머지

    if NofTimes == 0 and remainder > 0: # 총 페이지가 10보다 작을 떄
        for n in range(remainder):
            p = n + 1
            url = driver.current_url
            driver.get(url)
            time.sleep(2)
            crawling()
            Save()            # 한 페이지마다 저장
            if n == remainder-1:
                break
            else:
                page(p)

    elif NofTimes == 1 and remainder == 0: # 총 페이지가 딱 10일 때
        for n in range(10):
            p = n + 1
            url = driver.current_url
            driver.get(url)
            time.sleep(2)
            crawling()
            Save()            
            if n == 9:
                break
            else:
                page(p)

    elif NofTimes == 1 and remainder != 0: # 총 페이지가 10보다 큰데 20보다 작을 떄
        for n in range(10):
            p = n + 1
            url = driver.current_url
            driver.get(url)
            time.sleep(2)
            crawling()
            Save()            
            page(p)

        for n in range(remainder):
            p = n + 2
            url = driver.current_url
            driver.get(url)
            time.sleep(2)
            crawling()
            Save()            
            if n == remainder-1:
                break
            else:
                page(p)

    elif NofTimes >= 2 and remainder != 0: # 총 페이지가 20보다 크고, 10의 배수가 아님
        for n in range(10):
            p = n + 1
            url = driver.current_url
            driver.get(url)
            time.sleep(2)
            crawling()
            Save()            
            page(p)

        for a in range(NofTimes):
            for n in range(10):
                p = n + 2
                url = driver.current_url
                driver.get(url)
                time.sleep(2)
                crawling()
                Save()
                if a == NofTimes-1 and n == remainder-1:
                    break
                else:
                    page(p)

    elif NofTimes >= 2 and remainder == 0: # 총 페이지가 20보다 크고, 10의 배수
        for n in range(10):
            p = n + 1
            url = driver.current_url
            driver.get(url)
            time.sleep(2)
            crawling()
            Save()
            page(p)

        for a in range(NofTimes):
            for n in range(10):
                p = n + 2
                url = driver.current_url
                driver.get(url)
                time.sleep(2)
                crawling()
                Save()
                if a == NofTimes-1 and n == 9:
                    break
                else:
                    page(p)

def Save():  # 결과 출력
    result = pd.DataFrame(alist, columns=['뮤지컬명', '장르', '장소', '캐스팅'])
    result.index = result.index + 1
    result.to_csv('뮤지컬 목록 - 2019.csv', encoding='cp949')


#현재 공연중=1,개막 예정=2,2019년=3,2018=4, ... , 2007년 이하=15
Run(3)
Save()
```
