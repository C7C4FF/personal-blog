---
layout: post
title: "0624 크롤링 정리"
subtitle: "24일 코드 정리."
date: 2020-06-24 13:00:00 +0900
background: '/img/posts/06.jpg'
categories: ['programming', 'python']
---

# 0624

[뮤지컬 목록 수정본.csv](0624%204a82c7e7892240b792c0fe0655dde4f1/__.csv)

수정내역

1. 크롤링 함수 내에서 리스트 길이 3 (장르, 일시, 장소만 적힌 뮤지컬) 받아오기
2. For 문에서 alist = crawling() 을 crawling() 으로 변경



```python
def crawling():
    for i in range(1, 16):
        j = 1 + (i * 2)
        title = driver.find_element_by_xpath('//*[@id="contents"]/div[2]/table/tbody/tr[11]/td/table/tbody/tr[' + str(j) + ']/td/table/tbody/tr/td[1]/table/tbody/tr/td[3]/table/tbody/tr[1]/td/b/font/a')
        tmp = driver.find_element_by_xpath('//*[@id="contents"]/div[2]/table/tbody/tr[11]/td/table/tbody/tr[' + str(j) + ']/td/table/tbody/tr/td[1]/table/tbody/tr/td[3]/table/tbody/tr[2]/td')
        tmp_list = tmp.text.split("\n")
        genre = tmp_list[0][7:]
        date = tmp_list[1]
        loca = tmp_list[2][5:]
        if len(tmp_list) == 3: # 장르,일시,장소만 있는 곳도 받아오게 추가
            cast = "none"
            staff = "none"
            alist.append([title.text, genre, loca, cast])
        elif len(tmp_list) == 4 and tmp_list[3][0:2] == "출연":
            cast = tmp_list[3][5:]
            staff = "none"
            alist.append([title.text, genre, loca, cast])
        elif len(tmp_list) == 4 and tmp_list[3][0:5] == "Staff":
            cast = "none"
            staff = tmp_list[3][7:]
            alist.append([title.text, genre, loca, cast])
        elif len(tmp_list) == 5 and tmp_list[3][0:2] == "출연":
            cast = tmp_list[3][5:]
            staff = tmp_list[4][7:]
            alist.append([title.text, genre, loca, cast])
    return alist
```

```python
for n in range(10):
    p = n + 1
    url = driver.current_url
    driver.get(url)
    time.sleep(2)
	    crawling() #alist = crwaling()을 수정
    page(p)

for a in range(16):
    for n in range(10):
        p = n + 2
        url = driver.current_url
        driver.get(url)
        time.sleep(2)
        crawling() #alist = crwaling()을 수정
        if a == 15 and n == 9:
            pass
        else:
            page(p)
```

---

전체 코드

```python
from selenium import webdriver
import pandas as pd
import time

options = webdriver.ChromeOptions()
options.add_argument('headless')
options.add_argument("--disable-gpu")
driver = webdriver.Chrome('/Users/beom/Documents/chromedriver', chrome_options=options)
driver.get("http://www.playdb.co.kr/playdb/playdblist.asp?sReqMainCategory=000001&sReqSubCategory=&sReqDistrict=&sReqTab=2&sPlayType=1&sStartYear=2019&sSelectType=3")

def page(pagevalue):
    pages = driver.find_element_by_xpath('//*[@id="contents"]/div[2]/table/tbody/tr[11]/td/table/tbody/tr[35]/td/a[' + str(pagevalue) + ']')
    pages.click()

alist = []

def crawling():
    for i in range(1, 16):
        j = 1 + (i * 2)
        title = driver.find_element_by_xpath('//*[@id="contents"]/div[2]/table/tbody/tr[11]/td/table/tbody/tr[' + str(j) + ']/td/table/tbody/tr/td[1]/table/tbody/tr/td[3]/table/tbody/tr[1]/td/b/font/a')
        tmp = driver.find_element_by_xpath('//*[@id="contents"]/div[2]/table/tbody/tr[11]/td/table/tbody/tr[' + str(j) + ']/td/table/tbody/tr/td[1]/table/tbody/tr/td[3]/table/tbody/tr[2]/td')
        tmp_list = tmp.text.split("\n")
        genre = tmp_list[0][7:]
        date = tmp_list[1]
        loca = tmp_list[2][5:]
        if len(tmp_list) == 3:
            cast = "none"
            staff = "none"
            alist.append([title.text, genre, loca, cast])
        elif len(tmp_list) == 4 and tmp_list[3][0:2] == "출연":
            cast = tmp_list[3][5:]
            staff = "none"
            alist.append([title.text, genre, loca, cast])
        elif len(tmp_list) == 4 and tmp_list[3][0:5] == "Staff":
            cast = "none"
            staff = tmp_list[3][7:]
            alist.append([title.text, genre, loca, cast])
        elif len(tmp_list) == 5 and tmp_list[3][0:2] == "출연":
            cast = tmp_list[3][5:]
            staff = tmp_list[4][7:]
            alist.append([title.text, genre, loca, cast])
    return alist

for n in range(10):
    p = n + 1
    url = driver.current_url
    driver.get(url)
    time.sleep(2)
    crawling()
    page(p)

for a in range(16):
    for n in range(10):
        p = n + 2
        url = driver.current_url
        driver.get(url)
        time.sleep(2)
        crawling()
        if a == 15 and n == 9:
            pass
        else:
            page(p)

# print(alist)
result = pd.DataFrame(alist, columns=['뮤지컬명', '장르', '장소', '캐스팅'])
result.index = result.index + 1
result.to_csv('뮤지컬 목록 수정본.csv', encoding='cp949')
print(result.head(5))
```
