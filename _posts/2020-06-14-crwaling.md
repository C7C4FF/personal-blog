---
layout: post
title: "0614 크롤링 정리"
subtitle: "14일 코드 정리"
date: 2020-06-15 13:00:00 +0900
background: '/img/posts/06.jpg'
categories: ['Programming', 'Python']
---

# 0614

- [ ]  driver가 첫 주소를 두번 띄우는 거 고치기
- [x]  페이지 변수 p int 빼서 돌려보기
- [x]  584번까지 밖에 못 가져오는거 고치기...

예시)

[뮤지컬 목록.csv](0614%204e7f6a67460440c4a29d15428df57c22/_.csv)

[오류](0614%204e7f6a67460440c4a29d15428df57c22/%E1%84%8B%E1%85%A9%E1%84%85%E1%85%B2%20a7cf114193964f779858f5755a619872.md)

```python
from selenium import webdriver
import pandas as pd
import time
```

```python
# 크롬드라이버 옵션
options = webdriver.ChromeOptions()
options.add_argument('headless') # 창 띄우지 않기
options.add_argument("--disable-gpu") # gpu 가속 사용 x
driver = webdriver.Chrome('/Users/beom/Documents/chromedriver', chrome_options=options)
driver.get("http://www.playdb.co.kr/playdb/playdblist.asp?sReqMainCategory=000001&sReqSubCategory=&sReqDistrict=&sReqTab=2&sPlayType=1&sStartYear=2019&sSelectType=3")
	#2019-2020 작품
```

```python
# 페이지 넘기는 함수
def page(pagevalue):
    pages = driver.find_element_by_xpath('//*[@id="contents"]/div[2]/table/tbody/tr[11]/td/table/tbody/tr[35]/td/a[' + str(pagevalue) + ']')
    pages.click()
```

```python
# 크롤링 함수
alist = []

def crawling():
    for i in range(1, 16): #한 페이지에 15개의 작품이 있어서 15번
        j = 1 + (i * 2) # 한 작품당 3 -> 5 -> ... -> 31 로 2씩 증가
        title = driver.find_element_by_xpath('//*[@id="contents"]/div[2]/table/tbody/tr[11]/td/table/tbody/tr[' + str(j) + ']/td/table/tbody/tr/td[1]/table/tbody/tr/td[3]/table/tbody/tr[1]/td/b/font/a')
        tmp = driver.find_element_by_xpath('//*[@id="contents"]/div[2]/table/tbody/tr[11]/td/table/tbody/tr[' + str(j) + ']/td/table/tbody/tr/td[1]/table/tbody/tr/td[3]/table/tbody/tr[2]/td')
        tmp_list = tmp.text.split("\n")
        genre = tmp_list[0][7:] # "장르 : " 빼고 가져오기
        date = tmp_list[1]
        loca = tmp_list[2][5:] # "출연 : " 빼고 가져오기
        if len(tmp_list) == 4 and tmp_list[3][0:2] == "출연": #'제목, 기간, 장소, 출연'인 경우
            cast = tmp_list[3][5:]
            staff = "none"
            alist.append([title.text, genre, loca, cast])
        elif len(tmp_list) == 4 and tmp_list[3][0:5] == "Staff": #'제목, 기간, 장소, 스태프'
            cast = "none"
            staff = tmp_list[3][7:]
            alist.append([title.text, genre, loca, cast])
        elif len(tmp_list) == 5 and tmp_list[3][0:2] == "출연": #'제목, 기간, 장소, 출연, 스태프'
            cast = tmp_list[3][5:]
            staff = tmp_list[4][7:]
            alist.append([title.text, genre, loca, cast])
    return alist
```

```python
# 1-10페이지 다음으로 넘기는 변수가 2부터라서 p = n + 1
for n in range(10):
    p = n + 1
    url = driver.current_url
    driver.get(url)
    time.sleep(2) #url 로딩 될때까지 기다리기
    alist = crawling()
    page(p)

# 11-170 페이지 / 11페이지부터는 왼쪽으로 넘기는 키가 있어서 3부터 시작. p + 2
for a in range(16): # 10단위 페이지 넘기기
    for n in range(10): # 1단위 페이지 넘기기
        p = n + 2
        url = driver.current_url
        driver.get(url)
        time.sleep(2)
        alist=crwaling()
        if a == 15 and n == 9: # 170페이지에서 페이지 넘기기 실행 x
            pass
        else:
            page(p)
```

```python
#csv 저장
result = pd.DataFrame(alist, columns=['뮤지컬명', '장르', '장소', '캐스팅'])
result.index = result.index + 1
result.to_csv('뮤지컬 목록.csv', encoding='cp949')
print(result.head(5))
```

---

전체 코드

```python
from selenium import webdriver
import pandas as pd
import time

options = webdriver.ChromeOptions()
options.add_argument('headless')
options.add_argument("--disable-gpu")
driver = webdriver.Chrome('/Users/beom/Documents/chromedriver', chrome_options=options)
driver.get("http://www.playdb.co.kr/playdb/playdblist.asp?sReqMainCategory=000001&sReqSubCategory=&sReqDistrict=&sReqTab=2&sPlayType=1&sStartYear=2019&sSelectType=3")

def page(pagevalue):
    pages = driver.find_element_by_xpath('//*[@id="contents"]/div[2]/table/tbody/tr[11]/td/table/tbody/tr[35]/td/a[' + str(pagevalue) + ']')
    pages.click()

alist = []

def crawling():
    for i in range(1, 16):
        j = 1 + (i * 2)
        title = driver.find_element_by_xpath('//*[@id="contents"]/div[2]/table/tbody/tr[11]/td/table/tbody/tr[' + str(j) + ']/td/table/tbody/tr/td[1]/table/tbody/tr/td[3]/table/tbody/tr[1]/td/b/font/a')
        tmp = driver.find_element_by_xpath('//*[@id="contents"]/div[2]/table/tbody/tr[11]/td/table/tbody/tr[' + str(j) + ']/td/table/tbody/tr/td[1]/table/tbody/tr/td[3]/table/tbody/tr[2]/td')
        tmp_list = tmp.text.split("\n")
        genre = tmp_list[0][7:]
        date = tmp_list[1]
        loca = tmp_list[2][5:]
        if len(tmp_list) == 4 and tmp_list[3][0:2] == "출연":
            cast = tmp_list[3][5:]
            staff = "none"
            alist.append([title.text, genre, loca, cast])
        elif len(tmp_list) == 4 and tmp_list[3][0:5] == "Staff":
            cast = "none"
            staff = tmp_list[3][7:]
            alist.append([title.text, genre, loca, cast])
        elif len(tmp_list) == 5 and tmp_list[3][0:2] == "출연":
            cast = tmp_list[3][5:]
            staff = tmp_list[4][7:]
            alist.append([title.text, genre, loca, cast])
    return alist

for n in range(10):
    p = n + 1
    url = driver.current_url
    driver.get(url)
    time.sleep(2)
    alist = crawling()
    page(int(p))

for a in range(16):
    for n in range(10):
        p = n + 2
        url = driver.current_url
        driver.get(url)
        time.sleep(2)
        alist=crwaling()
        if a == 15 and n == 9:
            pass
        else:
            page(int(p))

result = pd.DataFrame(alist, columns=['뮤지컬명', '장르', '장소', '캐스팅'])
result.index = result.index + 1
result.to_csv('뮤지컬 목록.csv', encoding='cp949')
print(result.head(5))
```
